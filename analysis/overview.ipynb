{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a538a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "from datetime import datetime\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"credentials_gpe-analytics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee00889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = pd.read_json('../data/predictions/final_preds/all_preds.json',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44cc5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "gcs_folder = f'project-google-arxr-analytics-{today}'\n",
    "gcs_dataset = gcs_folder.replace(\"-\", \"_\")\n",
    "gcs_bucket = 'mm-gpe-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8c373",
   "metadata": {},
   "source": [
    "# What\n",
    "\n",
    "\n",
    "\n",
    "1. New twitter/news/insta needs combined with previous table \n",
    "CREATE new_updated_table\n",
    "SELECT * FROM current_table\n",
    "JOIN new_table \n",
    "WHERE current_table.updated_source=Reddit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f36c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = pd.read_json('../data/predictions/final_preds/all_preds.json',orient='records',lines=True)\n",
    "new_preds = all_preds.loc[~all_preds['Page Type'].isna()].reset_index(drop=True) # Remove all Reddit values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional\n",
    "old_preds = pd.read_json('../data/predictions/final_preds/old-preds.json',orient='records',lines=True)\n",
    "old_preds = old_preds.loc[old_preds['Page Type'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a0043d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_preds = pd.concat([new_preds, old_preds]).reset_index(drop=True)\n",
    "final_preds = pd.concat([new_preds]).reset_index(drop=True)\n",
    "final_preds.to_json(f\"../data/predictions/final_preds/{today}_final_preds.json\",orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96d17ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "Query Id\n",
      "Query Name\n",
      "Title\n",
      "text\n",
      "Full Text\n",
      "uid\n",
      "Domain\n",
      "Sentiment\n",
      "Emotion\n",
      "Page Type\n",
      "Language\n",
      "Country Code\n",
      "Continent Code\n",
      "Continent\n",
      "Country\n",
      "Region Code\n",
      "Region\n",
      "City Code\n",
      "Account Type\n",
      "Assignment\n",
      "Avatar\n",
      "Category Details\n",
      "Checked\n",
      "City\n",
      "Display URLs\n",
      "Expanded URLs\n",
      "Facebook Author ID\n",
      "Facebook Comments\n",
      "Facebook Likes\n",
      "Facebook Role\n",
      "Facebook Shares\n",
      "Facebook Subtype\n",
      "Full Name\n",
      "Gender\n",
      "Hashtags\n",
      "Impact\n",
      "Impressions\n",
      "Instagram Comments\n",
      "Instagram Followers\n",
      "Instagram Following\n",
      "Instagram Likes\n",
      "Instagram Posts\n",
      "Interest\n",
      "Last Assignment Date\n",
      "Latitude\n",
      "Location Name\n",
      "Longitude\n",
      "Media Filter\n",
      "Media URLs\n",
      "Mentioned Authors\n",
      "Priority\n",
      "Professions\n",
      "Resource Id\n",
      "Short URLs\n",
      "Starred\n",
      "Status\n",
      "Subtype\n",
      "Tags\n",
      "Thread Author\n",
      "Thread Created Date\n",
      "Thread Entry Type\n",
      "Thread Id\n",
      "Thread URL\n",
      "Total Monthly Visitors\n",
      "Twitter Author ID\n",
      "Twitter Channel Role\n",
      "Twitter Followers\n",
      "Twitter Following\n",
      "Twitter Reply Count\n",
      "Twitter Reply to\n",
      "Twitter Retweet of\n",
      "Twitter Retweets\n",
      "Twitter Likes\n",
      "Twitter Tweets\n",
      "Twitter Verified\n",
      "Reddit Score\n",
      "Reddit Score Upvote Ratio\n",
      "Reddit Author Karma\n",
      "Reddit Author Awardee Karma\n",
      "Reddit Author Awarder Karma\n",
      "Reddit Comments\n",
      "Subreddit\n",
      "Subreddit Subscribers\n",
      "Sina Weibo Post Count\n",
      "Sina Weibo Favourites Count\n",
      "Sina Weibo Followers\n",
      "Sina Weibo Following\n",
      "Sina Weibo Bi Followers\n",
      "Sina Weibo Raw Location\n",
      "Engagement Score\n",
      "AI Image Generation\n",
      "Android Scorecard Trending Topics\n",
      "Chrome EOY Review | Competitors\n",
      "Chrome: Promote Key Topics\n",
      "Google P&E X-Brand Brand Pulse Categories\n",
      "K-pop Exclusion\n",
      "P&E Verticals 2020\n",
      "Switching Snapshot: Competitors\n",
      "Switching Snapshot: Platforms\n",
      "Switching Snapshot: SPICES filters\n",
      "tag\n",
      "App store Robinhood Filter\n",
      "source\n",
      "llm_label\n",
      "query\n",
      "date\n",
      "author\n",
      "engagements\n",
      "updated_source\n",
      "link_url\n"
     ]
    }
   ],
   "source": [
    "final_preds = pd.read_json(f'../data/predictions/final_preds/{today}_final_preds.json',orient='records',lines=True)\n",
    "\n",
    "def all_preds_configuration(df):    \n",
    "    # Change date column to timestamp\n",
    "    if 'date' in df:\n",
    "        df['date'] = df['date'].combine_first(df['Date'])\n",
    "    else:\n",
    "        df['date']  = df['Date']\n",
    "\n",
    "    if 'author' in df:\n",
    "        df['author'] = df['author'].combine_first(df['Author'])\n",
    "    else:\n",
    "        df['author'] = df['Author']\n",
    "\n",
    "    # #Combine author for the news \n",
    "    df['author'] = df['author'].combine_first(df['Domain'])\n",
    "        \n",
    "    # # Now convert timestamp to string '2022-12-12'\n",
    "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # # Twitter\n",
    "    df['Engagements'] = df.apply(lambda x: x['Twitter Likes'] + x['Twitter Retweets'],axis=1)\n",
    "    # # Reddit\n",
    "    if 'redditScore' in df:\n",
    "       df['engagements'] = df['redditScore'].combine_first(df['Engagements'])\n",
    "    else:\n",
    "       df['engagements'] = df['Engagements']\n",
    "    \n",
    "    # # Create updated source column\n",
    "    df['updated_source'] = df.apply(lambda x: x['Page Type'] if x['Page Type'] else 'reddit',axis=1)\n",
    "    \n",
    "    #df['link_url'] = df.apply(lambda x: x.uid if x.updated_source!='reddit' else x.url,axis=1)\n",
    "    df['link_url'] = df['uid']\n",
    "    bad_colums = [\n",
    "        \"Reach (new)\", \n",
    "        \"Google AR/VR: Companies (Parent - products)\",\n",
    "        \"[Chrome AI Snap] Browsers\",\n",
    "        \"[Chrome AI Snap] ChatGPT vs Bard\",\n",
    "        \"[Chrome AI Snap] Search Engines\",\n",
    "        \"Engagements\",\n",
    "        \"Date\",\n",
    "        \"Author\",\n",
    "        \"Google AR/VR: Categories\",\n",
    "        \"Google AR/VR: Companies\",\n",
    "        \"Google AR/VR: Companies (Parent - products)\",\n",
    "        \"Google AR/VR: Events\",\n",
    "        \"Google AR/VR: Glasses\",\n",
    "        \"Google AR/VR: Headsets\"]\n",
    "    df.drop(columns=bad_colums, inplace=True)\n",
    "    \n",
    "    return df \n",
    "\n",
    "updated_preds = all_preds_configuration(final_preds)\n",
    "cols = updated_preds.columns\n",
    "cols.sort_values()\n",
    "for x in cols:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec22257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Table \n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0c59944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'project_google_arxr_analytics_20240619' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define table name, in format dataset.table_name\n",
    "dataset_id = f'{gcs_folder.replace(\"-\",\"_\")}'\n",
    "table = f'{dataset_id}.ar_xr_predictions_updated'\n",
    "\n",
    "# Check if the dataset exists\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "try:\n",
    "    client.get_dataset(dataset_ref, retry=bigquery.DEFAULT_RETRY)\n",
    "except Exception as e:\n",
    "    dataset = client.create_dataset(dataset_id)\n",
    "    print(f\"Dataset '{dataset.dataset_id}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Dataset '{dataset_id}' already exists.\")\n",
    "\n",
    "# Load data to BQ\n",
    "job = client.load_table_from_dataframe(updated_preds, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac8227",
   "metadata": {},
   "source": [
    "# Do Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c9d0325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'uid', 'source', 'date'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing keys \n",
    "{\"text\":\"Would you prefer for Apple to adapt to the industry standard of RCS? #fyp #android #apple #iphone #samsung #google #ipager #phone #greenbubble #bluebubble #rcs #androidvsiphone #getthemessage\",\"uid\":\"https:\\/\\/www.tiktok.com\\/@redirect-to\\/video\\/7287566836936346912\",\"source\":\"tiktok\",\"date\":\"2024-01-04\"}.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22a83cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'author', 'engagements', 'link', 'uid', 'date', 'queryName', 'query_title', 'subreddit', 'sentiment'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label map keys \n",
    "{\"source\": \"Platform\", \"author\": \"NULL\", \"engagements\": \"NULL\", \"link\": \"Video_URL\", \"uid\": \"Video_URL\", \"date\": \"date\", \"queryName\": \"NULL\", \"query_title\": \"Video_Title\", \"subreddit\": \"NULL\", \"sentiment\": \"NULL\"}.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "036abfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://preprocessed_data.json [Content-Type=application/json]...\n",
      "- [1 files][481.9 KiB/481.9 KiB]                                                \n",
      "Operation completed over 1 objects/481.9 KiB.                                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = final_preds[['date','text','uid','source']]\n",
    "preprocessed_data.to_json('preprocessed_data.json',orient='records',lines=True)\n",
    "os.system(f'gsutil cp preprocessed_data.json gs://{gcs_bucket}/{gcs_folder}/preprocessed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41ebdd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://data_label_map.json [Content-Type=application/json]...\n",
      "/ [1 files][  195.0 B/  195.0 B]                                                \n",
      "Operation completed over 1 objects/195.0 B.                                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'source':'source',\n",
    "            'author':'author',\n",
    "            'engagements':'NULL',\n",
    "            'link':'NULL',\n",
    "            'uid':'uid',\n",
    "            'date':'date',\n",
    "            'queryName':'NULL',\n",
    "            'query_title':'NULL',\n",
    "            'subreddit':'NULL',\n",
    "            'sentiment':'NULL'}\n",
    "\n",
    "\n",
    "with open('data_label_map.json','w') as f:\n",
    "    json.dump(label_map,f)\n",
    "    \n",
    "os.system(f'gsutil cp data_label_map.json gs://{gcs_bucket}/{gcs_folder}/label_maps/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efca5257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://raw_data.json [Content-Type=application/json]...\n",
      "\\ [1 files][  5.9 MiB/  5.9 MiB]                                                \n",
      "Operation completed over 1 objects/5.9 MiB.                                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.to_json('raw_data.json',orient='records',lines=True)\n",
    "os.system(f'gsutil cp raw_data.json gs://{gcs_bucket}/{gcs_folder}/raw/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73e302",
   "metadata": {},
   "source": [
    "# Run analytics [pipeline](https://console.cloud.google.com/vertex-ai/pipelines/locations/us-central1/runs/pipeline-googlae-arxr-analytics-20240427-run-1?project=gpe-analytics) to generate additional tables\n",
    "\n",
    "* redditnlp_output\n",
    "* custom_nlp_tags\n",
    "* clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec82e47",
   "metadata": {},
   "source": [
    "# Then, combine tables to create report dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555320dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define table name, in format dataset.table_name\n",
    "dataset_id = f'{gcs_folder.replace(\"-\",\"_\")}'\n",
    "    \n",
    "create_dashboard = \"\"\"\n",
    "CREATE TABLE `{dataset_id}.view_dashboard_final_update` as SELECT \n",
    "    * \n",
    "    FROM \n",
    "    (\n",
    "        SELECT \n",
    "        DISTINCT LEFT(text, 300) AS text, \n",
    "        uid, \n",
    "        category_1, \n",
    "        probability_1, \n",
    "        category_2, \n",
    "        probability_2, \n",
    "        subreddit_category_1,\n",
    "        subreddit_probability_1\n",
    "        FROM `{dataset_id}.redditnlp_output`\n",
    "    )  \n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "        updated_source ,link_url,uid,date_,query, tag, llm_label, engagements_ \n",
    "        FROM `{dataset_id}.ar_xr_predictions_table_updated`) USING (uid)\n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "        * \n",
    "        FROM `{dataset_id}.clusters`) USING (uid) \n",
    "    LEFT JOIN (\n",
    "        SELECT \n",
    "        * \n",
    "        FROM `{dataset_id}.custom_nlp_tags`) USING (uid) \n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "client.query(create_dashboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
